# !pip install numpy
# !pip install pandas
# !pip install matplotlib
# !pip install wordcloud
# !pip install mlxtend
# !pip install nltk
# !pip install scikit-learn
# !pip install seaborn
# !pip install unidecode
# !pip install plotly


import csv
import os
import re
import string
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from mlxtend.frequent_patterns import fpgrowth
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
import warnings
import unidecode
import plotly.express as px

warnings.filterwarnings("ignore")
nltk.download('stopwords')
from nltk.stem.porter import PorterStemmer


def lower_clean(text):
 text = text.lower()
 text = re.sub(r'[^a-z\s]', '', text) # Apenas letras e espaços
 return text

def unidecode_text(text):
 return unidecode.unidecode(text)

def tokenize(text: str) -> list:
 return word_tokenize(text)

def remove_stopwords(tokens: list) -> list:
 stop_words = set(stopwords.words('english'))
 return [word for word in tokens if word not in stop_words]

def lemmatize(tokens: list) -> list:
 lemmatizer = WordNetLemmatizer()
 return [lemmatizer.lemmatize(word) for word in tokens]

def stemming(tokens: list) -> list:
 stemmer = PorterStemmer()
 return [stemmer.stem(word) for word in tokens]


def preprocess_pipeline(text: str) -> str:
    cleaned_text = lower_clean(text)
    unidecoded_text = unidecode_text(cleaned_text)
    tokens = tokenize(unidecoded_text)
    tokens_no_stopwords = remove_stopwords(tokens)
    lemmatized_tokens = lemmatize(tokens_no_stopwords) # testar stemming(tokens_no_stopwords)
    return ' '.join(lemmatized_tokens)


df = pd.read_csv("filmes.csv")
df = df.sample(frac=0.5, random_state=42)
df.info()


df['sinopse'] = df['sinopse'].fillna('')

df['processed_sinopse'] = df['sinopse'].apply(preprocess_pipeline)

df = df.rename(columns={'genres': 'old_genres'})
df['genres'] = df['old_genres'].apply(lambda x: x.split(',')[0] if isinstance(x, str) else x)


# Visualizar as 5 primeiras linhas
df[['sinopse', 'processed_sinopse']].head()


# Criar o vetorizador TF-IDF

vectorizer = TfidfVectorizer(max_features=100_000, ngram_range=(1, 3))
print("Generating TFIDF sparse matrix...")
X_tfidf = vectorizer.fit_transform(df['processed_sinopse'])

# Converter a matriz TF-IDF para um DataFrame para visualização
tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())

# Visualizar as primeiras linhas
tfidf_df.shape


# vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.9)

# print("Generating TFIDF sparse matrix...")
# X_tfidf = vectorizer.fit_transform(df["sinopse"])


X_tfidf


df.shape


svd = TruncatedSVD(n_components=6_000, random_state=42)
X_svd = svd.fit_transform(X_tfidf)

cum_variance = np.cumsum(svd.explained_variance_ratio_)
threshold = 0.7  # Defina o limite desejado

if np.any(cum_variance > threshold):
    idx = np.argmax(cum_variance > threshold)
else:
    # Caso o limiar não seja atingido
    idx = len(cum_variance) - 1  # Índice do maior valor
    print(f"Variância acumulada máxima disponível: {cum_variance[idx]:.4f}")



if idx == 0:
    idx = 4000

print(f"O número de componentes para explicar 70% da variância é {idx}")


svd = TruncatedSVD(n_components=idx, random_state=42)
X_svd = svd.fit_transform(X_tfidf)


df.shape


from sklearn.cluster import AgglomerativeClustering
m = AgglomerativeClustering(6, linkage = 'average')
m.fit(X_svd)


from sklearn.cluster import AgglomerativeClustering
m = AgglomerativeClustering(6, linkage = 'average')
m.fit(X_svd)

clusters_agg = m.fit_predict(X_svd)

df['clusters_agg'] = clusters_agg

df.plot('X', 'Y', kind = 'scatter', colormap='viridis', c = m.labels_,
                   colorbar = False, figsize = (6, 6))
plt.axis('square')
plt.axis('off');
plt.axis('square')
plt.axis('off');


error = np.zeros(11)
for k in range(1,10):
    kmeans = KMeans(init='k-means++', n_clusters = k, n_init = 10)
    kmeans.fit_predict(X_svd)
    error[k] = kmeans.inertia_
    print(f'k = {k} -> error = {error[k]}')


plt.plot(range(1, len(error)), error[1:], 'o-')
plt.xlabel('Number of clusters')
plt.title(r'$k$-means clustering performance of synthetic data')
plt.ylabel('Error (Inertia)');


from sklearn import metrics


def sc_evaluate_clusters(X, max_clusters, n_init, seed):
    s = np.zeros(max_clusters+1)
    s[0] = 0
    s[1] = 0
    for k in range(2, max_clusters+1):
        kmeans = KMeans(init='k-means++', n_clusters = k, n_init = n_init, random_state = seed)
        kmeans.fit_predict(X)
        s[k] = metrics.silhouette_score(X, kmeans.labels_, metric = 'euclidean')
        print(f'k = {k} -> silhouette score = {s[k]}')
    return s

s = sc_evaluate_clusters(X_svd, 15, 15, 42)
plt.plot(range(2, len(s)), s[2:], 'o-')
plt.xlabel('Number of Clusters')
plt.title('$k$-means clustering performance on synthetic data')
plt.ylabel('Silhouette Score');


from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

# Substitua X_tfidf por X_svd, sua matriz reduzida
range_n_clusters = range(2, 12)# [2, 3, 4, 5, 6]

for n_clusters in range_n_clusters:
    # Cria uma figura maior (ajuste conforme necessário)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(40, 20))  # Aumenta o tamanho para 20x10 polegadas

    # O 1º subplot é o gráfico de Silhouette
    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(X_svd) + (n_clusters + 1) * 10])

    # Inicializa o cluster KMeans
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X_svd)

    # Calcula a média do silhouette_score
    silhouette_avg = silhouette_score(X_svd, cluster_labels)
    print(f"For n_clusters = {n_clusters}, the average silhouette_score is: {silhouette_avg}")

    # Calcula os valores individuais do silhouette_score
    sample_silhouette_values = silhouette_samples(X_svd, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Coleta os valores de silhouette para o cluster i e os ordena
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(
            np.arange(y_lower, y_upper),
            0,
            ith_cluster_silhouette_values,
            facecolor=color,
            edgecolor=color,
            alpha=0.7,
        )

        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10

    ax1.set_title("The silhouette plot for the various clusters.", fontsize=16)
    ax1.set_xlabel("The silhouette coefficient values", fontsize=14)
    ax1.set_ylabel("Cluster label", fontsize=14)
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
    ax1.set_yticks([])  # Remove os rótulos do eixo Y
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2º subplot mostrando os clusters reais formados
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(
        X_svd[:, 0], X_svd[:, 1], marker=".", s=30, lw=0, alpha=0.7, c=colors, edgecolor="k"
    )

    # Marca os centros dos clusters
    centers = clusterer.cluster_centers_
    ax2.scatter(
        centers[:, 0],
        centers[:, 1],
        marker="o",
        c="white",
        alpha=1,
        s=400,  # Tamanho aumentado para maior destaque
        edgecolor="k",
    )

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker=f"${i}$", alpha=1, s=100, edgecolor="k")

    ax2.set_title("The visualization of the clustered data.", fontsize=16)
    ax2.set_xlabel("Feature space for the 1st feature", fontsize=14)
    ax2.set_ylabel("Feature space for the 2nd feature", fontsize=14)

    # Define o título geral da figura
    plt.suptitle(
        f"Silhouette analysis for KMeans clustering on data with n_clusters = {n_clusters}",
        fontsize=18,
        fontweight="bold",
    )

    # Mostra a figura
    plt.show()



num_clusters = 8
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters_kmeans = kmeans.fit_predict(X_svd)

df['cluster_kmeans'] = clusters_kmeans

# Calculando o silhouette score para avaliar a coesão dos clusters
score_kmeans = silhouette_score(X_svd, clusters_kmeans)
print("Silhouette Score (K-Means):", score_kmeans)


import plotly.express as px

# Criar um histograma separado para cada cluster
for cluster in df['cluster_kmeans'].unique():
    df_cluster = df[df['cluster_kmeans'] == cluster]  # Filtra os dados do cluster atual

    fig = px.histogram(
        df_cluster,
        x="genres",  # Agrupa por gênero
        title=f"Distribuição de Gêneros no Cluster {cluster}",
        text_auto=True
    )

    # Ajusta altura para melhor visualização
    fig.update_layout(height=600)

    fig.show()

