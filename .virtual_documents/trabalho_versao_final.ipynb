





























!python --version






# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1AeYgV89TmYvNC__RDXr8hS0P6WOsChWg' -O filmes.csv






# !pip install --upgrade pip

# !pip install numpy
# !pip install pandas
# !pip install matplotlib
# !pip install wordcloud
# !pip install nltk
# !pip install scikit-learn
# !pip install seaborn
# !pip install unidecode
# !pip install plotly
# !pip install mlxtend
# !pip install tqdm
# !pip install spacy
# !python -m spacy download en_core_web_sm






import math
import re
import warnings
from re import Pattern
from typing import Tuple

import matplotlib.cm as cm
import matplotlib.pyplot as plt
import nltk
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import spacy
import unidecode
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from numpy import linalg as LA
from plotly.subplots import make_subplots
from scipy.sparse import csgraph
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.neighbors import kneighbors_graph
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.cluster import SpectralClustering
from tqdm import tqdm
from wordcloud import WordCloud

tqdm.pandas()

stemmer = SnowballStemmer("english")

warnings.filterwarnings("ignore")
nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('wordnet')
nlp = spacy.load('en_core_web_sm')






df = pd.read_csv("./filmes.csv")
df = df.sample(frac=0.2, random_state=42)
original_df = df
df.drop(columns=['averageRating','numVotes','primaryTitle','startYear','runtimeMinutes','actors_names','directors_names'], inplace=True)





# Criando dicionarário de stopwords
NUMERAL_WORDS = {
    "zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine",
    "ten", "eleven", "twelve", "thirteen", "fourteen", "fifteen", "sixteen",
    "seventeen", "eighteen", "nineteen", "twenty", "thirty", "forty", "fifty",
    "sixty", "seventy", "eighty", "ninety", "hundred", "thousand", "million"
}

STOPWORDS = set(stopwords.words('english')).union(NUMERAL_WORDS)


# Funções de pré-processamento
def lower_clean(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    return text

def unidecode_text(text):
    return unidecode.unidecode(text)

def tokenize(text: str) -> list:
    return word_tokenize(text)

def remove_stopwords(tokens: list, stop_words_english: set) -> list:
    return [word for word in tokens if word not in stop_words_english]

def lemmatize(tokens: list) -> list:
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in tokens]

def stemmer_text(tokens: list) -> list:
    stemmer = SnowballStemmer("english")
    return [stemmer.stem(word) for word in tokens]


def lemmatize_with_spacy(text):
    txt = text
    if isinstance(text, list):
        txt = " ".join(text)
    doc = nlp(txt)
    return [token.lemma_ for token in doc]

def preprocess_pipeline(text: str) -> list:
    cleaned_text = lower_clean(text)
    unidecoded_text = unidecode_text(cleaned_text)
    tokens = tokenize(unidecoded_text)
    tokens_no_stopwords = remove_stopwords(tokens, STOPWORDS)

    return lemmatize_with_spacy(tokens_no_stopwords)





def get_n_gram_frequencies(texts: list, ngram_range: Tuple[int, int] = (2, 2)) -> list:
    vectorizer = CountVectorizer(ngram_range=ngram_range)
    X = vectorizer.fit_transform(texts)
    bigrams = vectorizer.get_feature_names_out()
    freqs = np.asarray(X.sum(axis=0)).ravel()
    freq_bigrams = list(zip(bigrams, freqs))
    freq_bigrams.sort(key=lambda x: x[1], reverse=True)
    return freq_bigrams

def build_n_gram_pattern(freq_min = 3, ngram_range: Tuple[int, int] = (2, 2)) -> Pattern[str]:
    freq_bigrams = get_n_gram_frequencies(texts=df['processed_sinopse'], ngram_range=ngram_range)
    valid_bigrams = [bigram for bigram, freq in freq_bigrams if freq >= freq_min]
    valid_bigrams.sort(key=len, reverse=True)
    return re.compile(r'\b(' + '|'.join(map(re.escape, valid_bigrams)) + r')\b')



df['processed_sinopse_tokens'] = df['sinopse'].progress_apply(preprocess_pipeline)
df['processed_sinopse'] = df['processed_sinopse_tokens'].progress_apply(lambda x: ' '.join(x))


df.head()





pattern = build_n_gram_pattern()

df['processed_sinopse'] = df['processed_sinopse'].progress_apply(
    lambda text: pattern.sub(lambda match: match.group(0).replace(' ', '_'), text)
)


df.head()


df_exploded = df['genres'].str.split(',').explode()
df_exploded = df_exploded.str.strip()

unique_labels, category_sizes = np.unique(df_exploded, return_counts=True)
true_k = unique_labels.shape[0]

print(f"{len(df['processed_sinopse'])} documents - {true_k} categories")








from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer

def build_tf_idf(data_to_proccess: list):
    vectorizer = TfidfVectorizer(
        max_df=0.5,
        min_df=4,
        ngram_range=(1, 3)
    )

    return vectorizer, vectorizer.fit_transform(data_to_proccess)


vectorizer, X_tfidf = build_tf_idf(df['processed_sinopse'])

print(f"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}")






print(f"{X_tfidf.nnz / np.prod(X_tfidf.shape):.4f}")











wordcloud = WordCloud(width=1000, height=500).generate(str(df['processed_sinopse']))
fig = px.imshow(wordcloud.to_array())

fig.update_layout(
    xaxis_visible=False,
    yaxis_visible=False,
    margin=dict(l=0, r=0, t=0, b=0)
)

fig.show()






genres=pd.value_counts(df.genres)

print(f"Existem {len(genres)} generos diferentes no dataset")
print(f"Existem {len(unique_labels)} generos únicos no dataset")


print('-' * 70)
print(genres)
print('-' * 70)
print(unique_labels)





qtd_top_genres = 6
top_genres = pd.DataFrame(genres[:qtd_top_genres]).reset_index()
top_genres.columns = ['genres', 'number_of_movies']
top_genres


fig = px.bar(top_genres,
             x="genres",
             y="number_of_movies",
             title=f"Top {qtd_top_genres} gêneros e suas frequências")

fig.update_layout(
    xaxis_title="Gêneros",
    yaxis_title="Número de Filmes",
    width=800,
    height=600
)
fig.show()





fig, axes = plt.subplots(nrows=(qtd_top_genres//2), ncols=3, figsize=(20, 20))
axes = axes.flatten()

for i, genre in enumerate(top_genres['genres'].tolist()[:9]):
    genre_df = df[df['genres'] == genre]
    wordcloud = WordCloud(width=800, height=800)\
                .generate(str(genre_df['processed_sinopse']))

    axes[i].imshow(wordcloud, interpolation='bilinear')
    axes[i].axis('off')
    axes[i].set_title(f"Nuvem de palavras por gênero: {genre}", fontsize=16)

for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()



genre_df = df[df['genres'] == genre]
genre_df


def flatten(list_of_items):
    flattened = []
    for item in list_of_items:
        if isinstance(item, list):
            flattened.extend(item)
        else:
            flattened.append(item)
    return flattened


genres_list = top_genres['genres'].tolist()[:qtd_top_genres]

fig = make_subplots(
    rows=(qtd_top_genres//2), cols=3,
    subplot_titles=[f'{genre}: Frequência top 10 palavras' for genre in genres_list]
)

for i, genre in enumerate(genres_list):
    genre_df = df[df['genres'] == genre]

    count = CountVectorizer(stop_words='english')
    docs = count.fit_transform(flatten(genre_df['processed_sinopse_tokens']))
    features = count.get_feature_names_out()

    word_count_df = pd.DataFrame(docs.toarray().sum(axis=0), index=features, columns=['count'])
    word_count_df = word_count_df.sort_values(by='count', ascending=False)
    top_words = word_count_df.head(10)

    trace = go.Bar(
        x=top_words['count'],
        y=top_words.index,
        orientation='h',
        marker_color='skyblue',

    )

    row = i // 3 + 1
    col = i % 3 + 1
    fig.add_trace(trace, row=row, col=col)

    fig.update_xaxes(title_text="Frequência", row=row, col=col, tickfont=dict(size=12))
    fig.update_yaxes(title_text="Palavras", row=row, col=col, tickfont=dict(size=12), autorange='reversed')

fig.update_layout(
    height=1200,
    width=1200,
    showlegend=False,
    title_text="Top gênero - distribuição de frequência das top 10 palavras por gênero"
)

fig.show()









def select_n_components(X_tfidf: np.ndarray, threshold: float = 0.85) -> int:
    max_components = X_tfidf.shape[1]
    inital_n_comps = max(2, round(max_components / 3))
    n_comps = inital_n_comps
    found = False

    while n_comps <= max_components:
        print(f"Testando com {n_comps} componentes...")
        svd = TruncatedSVD(n_components=n_comps)
        svd.fit(X_tfidf)
        cum_variance = np.cumsum(svd.explained_variance_ratio_)

        if cum_variance[-1] >= threshold:
            n_components = int(np.argmax(cum_variance >= threshold))
            found = True
            break
        else:
            n_comps += inital_n_comps

    if not found:
        n_components = max_components - 1
        print(f"Variância acumulada máxima disponível: {cum_variance[-1]:.4f}")

    print(f"Número de componentes selecionado: {n_components}")
    return n_components



n_components = select_n_components(X_tfidf)





def perform_lsa(X_tfidf: np.ndarray, n_components: int):
    lsa = make_pipeline(TruncatedSVD(n_components=n_components), Normalizer(copy=False))
    X_lsa = lsa.fit_transform(X_tfidf)
    explained_variance = lsa[0].explained_variance_ratio_.sum()
    return lsa, X_lsa, explained_variance


lsa, X_lsa, explained_variance = perform_lsa(X_tfidf, n_components)
print(f"Variância explicada da etapa SVD: {explained_variance * 100:.1f}%")




















def evaluate_kmeans_nclusters(n_clusters_range: range, X_svd: np.ndarray, labels: list):
    results = []
    for n in n_clusters_range:
        print(f"n_clusters: {n}")
        km = KMeans(n_clusters=n, n_init=5, max_iter=1000, init='k-means++')
        km.fit(X_svd)

        homogeneity   = metrics.homogeneity_score(labels, km.labels_)
        completeness  = metrics.completeness_score(labels, km.labels_)
        v_measure     = metrics.v_measure_score(labels, km.labels_)
        adjusted_rand = metrics.adjusted_rand_score(labels, km.labels_)
        silhouette    = metrics.silhouette_score(X_svd, km.labels_)

        results.append({
            "n_clusters": n,
            "Homogeneity": homogeneity,
            "Completeness": completeness,
            "V-measure": v_measure,
            "Adjusted Rand-Index": adjusted_rand,
            "Silhouette": silhouette
        })

    metric_names = ["Homogeneity", "Completeness", "V-measure", "Adjusted Rand-Index", "Silhouette"]
    df_long = pd.DataFrame(results).melt(id_vars='n_clusters', value_vars=metric_names,
                              var_name='Métrica', value_name='Valor')

    fig = px.line(
        df_long,
        x="n_clusters",
        y="Valor",
        color="Métrica",
        markers=True,
        title="KMeans: Métricas vs n_clusters"
    )
    fig.update_traces(texttemplate='%{y:.3f}', textposition='top center')
    # fig.update_layout(yaxis_range=[0, 0.1])
    fig.update_xaxes(dtick=1)
    fig.show()


evaluate_kmeans_nclusters(range(2, 28), X_lsa, df['genres'])





def silhouette_analysis_generic(X: np.ndarray, range_n_clusters: range, clustering_algo, **clust_kwargs):
    for n_clusters in range_n_clusters:
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_size_inches(24, 12)
        ax1.set_xlim([-0.01, 0.1])
        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

        clusterer = clustering_algo(n_clusters=n_clusters, **clust_kwargs)
        cluster_labels = clusterer.fit_predict(X)

        silhouette_avg = silhouette_score(X, cluster_labels)
        print(f"n_clusters = {n_clusters} Silhouette avg = {silhouette_avg}")
        sample_silhouette_values = silhouette_samples(X, cluster_labels)

        y_lower = 10
        for i in range(n_clusters):
            ith_silhouette_vals = sample_silhouette_values[cluster_labels == i]
            ith_silhouette_vals.sort()
            size_cluster_i = ith_silhouette_vals.shape[0]
            y_upper = y_lower + size_cluster_i
            color = cm.nipy_spectral(float(i) / n_clusters)
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                              0, ith_silhouette_vals,
                              facecolor=color, edgecolor=color, alpha=0.7)
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            y_lower = y_upper + 10

        ax1.set_title("Gráfico de Silhueta para n_clusters = " + str(n_clusters))
        ax1.set_xlabel("Valor do coeficiente de Silhueta")
        ax1.set_ylabel("Índice dos pontos")
        ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
        ax1.set_yticks([])

        # Clusters usando as duas primeiras features
        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
        ax2.scatter(X[:, 0], X[:, 1], marker=".", s=30, lw=0, alpha=0.7, c=colors, edgecolor="k")

        if hasattr(clusterer, "cluster_centers_"):
            centers = clusterer.cluster_centers_
            ax2.scatter(centers[:, 0], centers[:, 1], marker="o", c="white", alpha=1, s=200, edgecolor="k")
            for i, c in enumerate(centers):
                ax2.scatter(c[0], c[1], marker=f"${i}$", alpha=1, s=50, edgecolor="k")

        ax2.set_title("Visualização dos clusters")
        ax2.set_xlabel("Feature 1")
        ax2.set_ylabel("Feature 2")

        plt.suptitle(f"Análise de Silhueta para {clustering_algo.__name__} com n_clusters = {n_clusters}",
                     fontsize=14, fontweight="bold")
        plt.show()



# silhouette_analysis_generic(X_lsa, range(2, 10), KMeans, n_init=10, max_iter=300, init='k-means++')


n_clusters = 7





kmeans = KMeans(n_clusters=n_clusters, n_init=5, max_iter=1000, init='k-means++')
kmeans.fit(X_lsa)
df['cluster_kmeans'] = kmeans.labels_





def get_cluster_summary(df: pd.DataFrame, cluster_col: str = 'cluster_kmeans') -> pd.DataFrame:
    cluster_counts = df[cluster_col].value_counts().sort_index()
    cluster_percentages = df[cluster_col].value_counts(normalize=True).sort_index() * 100

    cluster_summary = pd.DataFrame({
        'Count': cluster_counts,
        'Percentage': cluster_percentages
    })

    return cluster_summary


cluster_summary = get_cluster_summary(df, 'cluster_kmeans')
cluster_summary





dominant_cluster = cluster_summary['Count'].idxmax()
print(f"Cluster mais dominante: {dominant_cluster}")

df_cluster_dominant = df[df['cluster_kmeans'] == dominant_cluster]

df_cluster_dominant.head()





# _, X_tfidf_dominant = build_tf_idf(df_cluster_dominant['processed_sinopse'])
# n_components_dominant = select_n_components(X_tfidf_dominant, threshold=0.8)
# _, X_lsa_dominant, _ = perform_lsa(X_tfidf_dominant, n_components_dominant)
#silhouette_analysis_generic(X_lsa_dominant, range(2, 16), KMeans, n_init=10, max_iter=300, init='k-means++')






def print_top_terms(kmeans, lsa, vectorizer, n_clusters: int, n_terms: int = 10):
    original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)
    order_centroids = original_space_centroids.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()

    top_terms_by_cluster = {}
    for i in range(n_clusters):
        print(f"Cluster {i}: ", end="")
        cluster_terms = []
        for ind in order_centroids[i, :n_terms]:
            term = terms[ind]
            print(f"{term} ", end="")
            cluster_terms.append(term)
        print()
        top_terms_by_cluster[i] = cluster_terms

    return order_centroids, top_terms_by_cluster


order_centroids, top_terms_by_cluster = print_top_terms(kmeans, lsa, vectorizer, n_clusters=n_clusters, n_terms=10)


def plot_cluster_genres_heatmap(df: pd.DataFrame, cluster_col: str = 'cluster_kmeans', genres_col: str = 'genres') -> None:
    df_exploded = df.copy()
    df_exploded[genres_col] = df_exploded[genres_col].str.split(',')
    df_exploded = df_exploded.explode(genres_col)
    df_exploded[genres_col] = df_exploded[genres_col].str.strip()

    ct = pd.crosstab(df_exploded[cluster_col], df_exploded[genres_col], normalize='index') * 100

    fig = px.imshow(
        ct,
        labels={'x': 'Gêneros', 'y': 'Cluster', 'color': 'Percentual (%)'},
        x=ct.columns,
        y=ct.index,
        text_auto=".1f",  # Exibe os valores com 1 casa decimal
        color_continuous_scale="YlGnBu"
    )

    fig.update_layout(title="Heatmap: Percentual de Gêneros por Cluster")
    fig.show()


plot_cluster_genres_heatmap(df, cluster_col='cluster_kmeans', genres_col='genres')


def plot_cluster_top_terms_heatmaps(
    df: pd.DataFrame,
    n_clusters: int,
    order_centroids,
    terms: list,
    words_to_print: int = 10,
    cluster_col: str = 'cluster_kmeans',
    genres_col: str = 'genres',
    text_col: str = 'processed_sinopse'
) -> None:
    top_terms_by_cluster = {}
    for i in range(n_clusters):
        top_indices = order_centroids[i, :words_to_print]
        top_terms_by_cluster[i] = [terms[ind] for ind in top_indices]

    max_genres = 0
    for cluster in range(n_clusters):
        cluster_df = df[df[cluster_col] == cluster].copy()
        cluster_df[genres_col] = cluster_df[genres_col].str.split(',')
        cluster_df = cluster_df.explode(genres_col)
        cluster_df[genres_col] = cluster_df[genres_col].str.strip()
        num_genres = len(cluster_df[genres_col].unique())
        if num_genres > max_genres:
            max_genres = num_genres

    n_cols = 3
    n_rows = math.ceil(n_clusters / n_cols)
    subplot_titles = [f"Cluster {i}" for i in range(n_clusters)]
    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=subplot_titles, vertical_spacing=0.05, horizontal_spacing=0.1)

    for cluster in range(n_clusters):
        cluster_df = df[df[cluster_col] == cluster].copy()
        cluster_df[genres_col] = cluster_df[genres_col].str.split(',')
        cluster_df = cluster_df.explode(genres_col)
        cluster_df[genres_col] = cluster_df[genres_col].str.strip()
        top_terms = top_terms_by_cluster[cluster]
        genre_term_counts = {}
        for _, row in cluster_df.iterrows():
            genre = row[genres_col]
            text = row[text_col].lower()
            if genre not in genre_term_counts:
                genre_term_counts[genre] = {term: 0 for term in top_terms}
            for term in top_terms:
                genre_term_counts[genre][term] += text.count(term.lower())

        df_counts = pd.DataFrame.from_dict(genre_term_counts, orient='index').fillna(0)
        df_counts.index.name = 'Genre'

        row_idx = (cluster // n_cols) + 1
        col_idx = (cluster % n_cols) + 1
        axis_id = (row_idx - 1) * n_cols + col_idx
        xaxis_key = "xaxis" if axis_id == 1 else f"xaxis{axis_id}"
        yaxis_key = "yaxis" if axis_id == 1 else f"yaxis{axis_id}"
        domain_x = fig.layout[xaxis_key].domain if xaxis_key in fig.layout else [0, 1]
        domain_y = fig.layout[yaxis_key].domain if yaxis_key in fig.layout else [0, 1]
        colorbar_x = domain_x[1]
        colorbar_y = (domain_y[0] + domain_y[1]) / 2
        len_legend = (1.3/n_clusters) if n_clusters > 3 else 0.8

        heatmap_trace = go.Heatmap(
            z = df_counts.values,
            x = list(df_counts.columns),
            y = list(df_counts.index),
            colorscale = 'YlGnBu',
            text = df_counts.values,
            texttemplate = "%{text:.1f}",
            colorbar = dict(
                title="Qtd",
                xanchor="left",
                x=colorbar_x,
                y=colorbar_y,
                yanchor="middle",
                len=len_legend
            )
        )
        fig.add_trace(heatmap_trace, row=row_idx, col=col_idx)
        fig.update_yaxes(range=[-0.5, max_genres - 0.5], row=row_idx, col=col_idx)


    fig.update_traces(textfont=dict(size=8))
    fig.update_layout(
        title="Heatmaps: Contagem dos Top Termos por Gênero para cada Cluster",
        height=500 * n_rows,
        showlegend=False
    )
    fig.update_xaxes(dtick=1)
    fig.show()


    plot_cluster_top_terms_heatmaps(df, n_clusters=n_clusters, order_centroids=order_centroids, terms=vectorizer.get_feature_names_out(), words_to_print=10)


def plot_cluster_top_terms_bars(
    df: pd.DataFrame,
    n_clusters: int,
    top_terms_by_cluster: dict,
    cluster_col: str = 'cluster_kmeans',
    genres_col: str = 'genres',
    text_col: str = 'processed_sinopse',
    width: int = 1000,
    height: int = 600
) -> None:
    for cluster in range(n_clusters):
        cluster_df = df[df[cluster_col] == cluster].copy()
        cluster_df[genres_col] = cluster_df[genres_col].str.split(',')
        cluster_df = cluster_df.explode(genres_col)
        cluster_df[genres_col] = cluster_df[genres_col].str.strip()
        top_terms = top_terms_by_cluster[cluster]
        genre_term_counts = {}
        for _, row in cluster_df.iterrows():
            genre = row[genres_col]
            text = row[text_col].lower()
            if genre not in genre_term_counts:
                genre_term_counts[genre] = {term: 0 for term in top_terms}
            for term in top_terms:
                genre_term_counts[genre][term] += text.count(term.lower())
        df_counts = pd.DataFrame.from_dict(genre_term_counts, orient='index').fillna(0)
        df_counts.index.name = 'Genre'
        df_counts = df_counts.reset_index()
        df_melt = df_counts.melt(id_vars='Genre', var_name='term', value_name='count')
        fig = go.Figure()
        unique_terms = df_melt['term'].unique()
        for term in unique_terms:
            df_term = df_melt[df_melt['term'] == term]
            fig.add_trace(
                go.Bar(
                    x=df_term['Genre'],
                    y=df_term['count'],
                    name=term,
                    text=df_term['count'],
                    hovertemplate='%{x}<br>' + term + ': %{y}<extra></extra>'
                )
            )
        fig.update_layout(
            barmode="stack",
            template="plotly_white",
            title=f"Cluster {cluster}: Contribuição dos Top Termos por Gênero",
            xaxis_title="Gênero",
            yaxis_title="Contagem de Ocorrências",
            width=width,
            height=height,
            legend=dict(
                x=0.8,
                y=0.9,
                xanchor='left',
                yanchor='top'
            )
        )
        fig.show()



plot_cluster_top_terms_bars(df=df, n_clusters=n_clusters, top_terms_by_cluster=top_terms_by_cluster, cluster_col='cluster_kmeans', genres_col='genres', text_col='processed_sinopse')








def evaluate_spectral_nclusters(n_clusters_range: range, X_svd: np.ndarray, labels: list, affinity: str, n_neighbors: int = 10):
    results = []
    for n in n_clusters_range:
        print(f"n_clusters: {n}")
        sc = SpectralClustering(
            n_clusters=n,
            assign_labels="discretize",
            affinity=affinity,
            n_neighbors=n_neighbors,
            n_jobs=-1
        )

        sc_labels = sc.fit_predict(X_svd)

        homogeneity   = metrics.homogeneity_score(labels, sc_labels)
        completeness  = metrics.completeness_score(labels, sc_labels)
        v_measure     = metrics.v_measure_score(labels, sc_labels)
        adjusted_rand = metrics.adjusted_rand_score(labels, sc_labels)
        silhouette    = metrics.silhouette_score(X_svd, sc_labels) if n > 1 else np.nan

        results.append({
            "n_clusters": n,
            "Homogeneity": homogeneity,
            "Completeness": completeness,
            "V-measure": v_measure,
            "Adjusted Rand-Index": adjusted_rand,
            "Silhouette": silhouette
        })

    metric_names = ["Homogeneity", "Completeness", "V-measure", "Adjusted Rand-Index", "Silhouette"]
    df_long = pd.DataFrame(results).melt(
        id_vars='n_clusters',
        value_vars=metric_names,
        var_name='Métrica',
        value_name='Valor'
    )

    fig = px.line(
        df_long,
        x="n_clusters",
        y="Valor",
        color="Métrica",
        markers=True,
        title=f"Spectral Clustering: Métricas vs n_clusters, n_neighbors: {n_neighbors}, affinity={affinity}"
    )
    fig.update_traces(texttemplate='%{y:.3f}', textposition='top center')
    fig.update_xaxes(dtick=1)
    fig.show()


# evaluate_spectral_nclusters(range(2, 16), X_lsa, df['genres'], affinity='nearest_neighbors', n_neighbors=18)


# evaluate_spectral_nclusters(range(2, 16), X_lsa, df['genres'], affinity='rbf', n_neighbors=18)


def evaluate_eigen_gap(X: np.ndarray, n_neighbors: int = 10, n_eigen: int = 20):
    G = kneighbors_graph(X, n_neighbors=n_neighbors, include_self=True)
    A = 0.5 * (G + G.T)
    L = csgraph.laplacian(A, normed=True).todense()

    eigenvalues, _ = LA.eigh(L)
    eigenvalues = np.real(eigenvalues)

    df_eigen = pd.DataFrame({
        "Índice": np.arange(1, n_eigen + 1),
        "Autovalor": eigenvalues[:n_eigen]
    })

    fig = px.scatter(df_eigen, x="Índice", y="Autovalor", title="Autovalores da Laplaciana Normalizada",
                     labels={"Índice": "Índice do autovalor", "Autovalor": "Autovalor"})
    fig.update_traces(mode='lines+markers')
    fig.show()

    return eigenvalues


eigenvalues = evaluate_eigen_gap(X_lsa, n_neighbors=18, n_eigen=20)





silhouette_analysis_generic(X_lsa, range(2, 18), SpectralClustering, affinity='nearest_neighbors', n_neighbors=18)





m = SpectralClustering(
    n_clusters=11,
    assign_labels="discretize",
    affinity='nearest_neighbors',
    n_neighbors=18,
    n_jobs=-1
)
m.fit(X_lsa)

df['cluster_spectral'] = m.labels_





cluster_summary = get_cluster_summary(df, 'cluster_spectral')
cluster_summary


def print_top_terms_spectral(spectral_labels, X_svd, lsa, vectorizer, n_clusters: int, n_terms: int = 10):
    spectral_labels = np.array(spectral_labels)
    centers = np.zeros((n_clusters, X_svd.shape[1]))
    for i in range(n_clusters):
        # Verifica se o cluster não está vazio
        if np.sum(spectral_labels == i) == 0:
            continue
        centers[i] = np.mean(X_svd[spectral_labels == i], axis=0)

    # Substitui eventuais NaN por zero
    centers = np.nan_to_num(centers)

    # Transforma os pseudo-centróides de volta para o espaço original
    original_space_centroids = lsa[0].inverse_transform(centers)

    # Ordena os termos para cada cluster de forma decrescente
    order_centroids = original_space_centroids.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()

    top_terms_by_cluster = {}
    for i in range(n_clusters):
        cluster_terms = []
        for ind in order_centroids[i, :n_terms]:
            term = terms[ind]
            cluster_terms.append(term)
        top_terms_by_cluster[i] = cluster_terms
        print(f"Cluster {i}: {' '.join(cluster_terms)}")

    return order_centroids, top_terms_by_cluster


order_centroids_spectral, top_terms_by_cluster_spectral = print_top_terms_spectral(m.labels_, X_lsa, lsa, vectorizer, n_clusters=n_clusters, n_terms=10)





plot_cluster_genres_heatmap(df, cluster_col='cluster_spectral', genres_col='genres')


def plot_cluster_top_terms_heatmaps_spectral(
    df: pd.DataFrame,
    n_clusters: int,
    X_svd: np.ndarray,
    lsa,
    vectorizer,
    words_to_print: int = 10,
    cluster_col: str = 'cluster_spectral',
    genres_col: str = 'genres',
    text_col: str = 'processed_sinopse'
) -> None:
    centers = np.zeros((n_clusters, X_svd.shape[1]))
    for i in range(n_clusters):
        centers[i] = X_svd[df[cluster_col] == i].mean(axis=0)
    original_space_centroids = lsa[0].inverse_transform(centers)
    order_centroids = original_space_centroids.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()

    top_terms_by_cluster = {}
    for i in range(n_clusters):
        cluster_terms = [terms[ind] for ind in order_centroids[i, :words_to_print]]
        print(f"Cluster {i}: {' '.join(cluster_terms)}")
        top_terms_by_cluster[i] = cluster_terms

    max_genres = 0
    for cluster in range(n_clusters):
        cluster_df = df[df[cluster_col] == cluster].copy()
        cluster_df[genres_col] = cluster_df[genres_col].str.split(',')
        cluster_df = cluster_df.explode(genres_col)
        cluster_df[genres_col] = cluster_df[genres_col].str.strip()
        num_genres = len(cluster_df[genres_col].unique())
        max_genres = max(max_genres, num_genres)

    n_cols = 3
    n_rows = math.ceil(n_clusters / n_cols)
    subplot_titles = [f"Cluster {i}" for i in range(n_clusters)]
    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=subplot_titles,
                        vertical_spacing=0.05, horizontal_spacing=0.1)

    for cluster in range(n_clusters):
        cluster_df = df[df[cluster_col] == cluster].copy()
        cluster_df[genres_col] = cluster_df[genres_col].str.split(',')
        cluster_df = cluster_df.explode(genres_col)
        cluster_df[genres_col] = cluster_df[genres_col].str.strip()
        top_terms = top_terms_by_cluster[cluster]
        genre_term_counts = {}
        for _, row in cluster_df.iterrows():
            genre = row[genres_col]
            text = row[text_col].lower()
            if genre not in genre_term_counts:
                genre_term_counts[genre] = {term: 0 for term in top_terms}
            for term in top_terms:
                genre_term_counts[genre][term] += text.count(term.lower())
        df_counts = pd.DataFrame.from_dict(genre_term_counts, orient='index').fillna(0)
        df_counts.index.name = 'Genre'
        row_idx = (cluster // n_cols) + 1
        col_idx = (cluster % n_cols) + 1
        axis_id = (row_idx - 1) * n_cols + col_idx
        xaxis_key = "xaxis" if axis_id == 1 else f"xaxis{axis_id}"
        yaxis_key = "yaxis" if axis_id == 1 else f"yaxis{axis_id}"
        domain_x = fig.layout[xaxis_key].domain if xaxis_key in fig.layout else [0, 1]
        domain_y = fig.layout[yaxis_key].domain if yaxis_key in fig.layout else [0, 1]
        colorbar_x = domain_x[1]
        colorbar_y = (domain_y[0] + domain_y[1]) / 2
        len_colorbar = (1.3 / n_clusters) if n_clusters > 3 else 0.8

        heatmap_trace = go.Heatmap(
            z=df_counts.values,
            x=list(df_counts.columns),
            y=list(df_counts.index),
            colorscale='YlGnBu',
            text=df_counts.values,
            texttemplate="%{text:.1f}",
            colorbar=dict(
                title="Qtd",
                xanchor="left",
                x=colorbar_x,
                y=colorbar_y,
                yanchor="middle",
                len=len_colorbar
            )
        )
        fig.add_trace(heatmap_trace, row=row_idx, col=col_idx)
        fig.update_yaxes(range=[-0.5, max_genres - 0.5], row=row_idx, col=col_idx)

    fig.update_traces(textfont=dict(size=8))
    fig.update_layout(
        title="Heatmaps: Contagem dos Top Termos por Gênero para cada Cluster (Spectral)",
        height=500 * n_rows,
        showlegend=False
    )
    fig.update_xaxes(dtick=1)
    fig.show()


plot_cluster_top_terms_heatmaps_spectral(df, n_clusters=n_clusters, X_svd=X_lsa, lsa=lsa,
                                            vectorizer=vectorizer, words_to_print=10)


plot_cluster_top_terms_bars(df=df, n_clusters=n_clusters, top_terms_by_cluster=top_terms_by_cluster, cluster_col='cluster_spectral', genres_col='genres', text_col='processed_sinopse')



